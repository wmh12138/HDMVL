{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "InoOrXkiO6sW",
        "outputId": "b6b8a59f-a38d-45e2-da03-484948ee674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/ETMC'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/ETMC/\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9snFZZPmO9WN",
        "outputId": "12a9200e-916e-485f-ab79-d2173af06b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vision-mamba\n",
            "  Downloading vision_mamba-0.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vision-mamba) (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vision-mamba) (2.4.1+cu121)\n",
            "Collecting zetascale (from vision-mamba)\n",
            "  Downloading zetascale-2.7.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vision-mamba) (2024.6.1)\n",
            "Collecting accelerate==0.33.0 (from zetascale->vision-mamba)\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting argparse<2.0.0,>=1.4.0 (from zetascale->vision-mamba)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting beartype==0.18.5 (from zetascale->vision-mamba)\n",
            "  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting bitsandbytes (from zetascale->vision-mamba)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install vision-mamba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6oHe1C3hFl_"
      },
      "source": [
        "#数据库部分\n",
        "\n",
        "1.   初始化项\n",
        "2.   安装\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5NKL5cChD9W"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data\n",
        "\n",
        "class DataProvider():\n",
        "\n",
        "    def __init__(self, cfg, dataset, batch_size=None, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        if batch_size is None:\n",
        "            batch_size = cfg.BATCH_SIZE\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=int(cfg.WORKERS),\n",
        "            drop_last=False)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i, data in enumerate(self.dataloader):\n",
        "            yield data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFIu2ZiwhTpD"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from torchvision.datasets.folder import make_dataset\n",
        "import sys\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import torch\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "import copy\n",
        "\n",
        "\n",
        "class AlignedConcDataset:\n",
        "\n",
        "    def __init__(self, cfg, data_dir=None, transform=None, labeled=True):\n",
        "        self.cfg = cfg\n",
        "        self.transform = transform\n",
        "        self.data_dir = data_dir\n",
        "        self.labeled = labeled\n",
        "\n",
        "        self.classes, self.class_to_idx = find_classes(self.data_dir)\n",
        "        #假设 self.classes 是一个包含类别名称的列表，如 ['cat', 'dog', 'bird']，那么运行这行代码后，self.int_to_class 将成为以下字典：{0: 'cat', 1: 'dog', 2: 'bird'}\n",
        "        self.int_to_class = dict(zip(range(len(self.classes)), self.classes))\n",
        "        #函数将返回一个包含文件路径和相应类别索引的元组的列表，并找到带有PNG后缀的文件。\n",
        "        self.imgs = make_dataset(self.data_dir, self.class_to_idx, 'png')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.labeled:\n",
        "            img_path, label = self.imgs[index]\n",
        "        else:\n",
        "            img_path = self.imgs[index]\n",
        "\n",
        "        #img_path 是一个包含文件路径的字符串，如 /path/to/images/image.jpg，那么 os.path.basename(img_path) 将返回 'image.jpg'，这是文件路径中的文件名部分。\n",
        "        img_name = os.path.basename(img_path)\n",
        "        AB_conc = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # split RGB and Depth as A and B\n",
        "        w, h = AB_conc.size\n",
        "        w2 = int(w / 2)\n",
        "        #按照宽度进行切割\n",
        "        if w2 > self.cfg.FINE_SIZE:\n",
        "            #使用了Pillow（或Python Imaging Library，PIL）库中的 crop 方法，并提供了一个矩形区域的坐标范围作为参数。这个矩形由左上角点 (0, 0) 和右下角点 (w2, h) 定\n",
        "            #Image.BICUBIC：这是一个插值方法的参数。在这里，它指定了使用双三次插值方法进\n",
        "            A = AB_conc.crop((0, 0, w2, h)).resize((self.cfg.LOAD_SIZE, self.cfg.LOAD_SIZE), Image.BICUBIC)\n",
        "            B = AB_conc.crop((w2, 0, w, h)).resize((self.cfg.LOAD_SIZE, self.cfg.LOAD_SIZE), Image.BICUBIC)\n",
        "        else:\n",
        "            A = AB_conc.crop((0, 0, w2, h))\n",
        "            B = AB_conc.crop((w2, 0, w, h))\n",
        "\n",
        "        if self.labeled:\n",
        "            sample = {'A': A, 'B': B, 'img_name': img_name, 'label': label}\n",
        "        else:\n",
        "            sample = {'A': A, 'B': B, 'img_name': img_name}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['A'] = self.transform(sample['A'])\n",
        "            sample['B'] = self.transform(sample['B'])\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class RandomCrop(transforms.RandomCrop):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "\n",
        "        #F.pad是pytorch内置的tensor扩充函数，便于对数据集图像或中间层特征进行维度扩充\n",
        "        if self.padding > 0:\n",
        "            A = F.pad(A, self.padding)\n",
        "            B = F.pad(B, self.padding)\n",
        "\n",
        "        # pad the width if needed\n",
        "        if self.pad_if_needed and A.size[0] < self.size[1]:\n",
        "            A = F.pad(A, (int((1 + self.size[1] - A.size[0]) / 2), 0))\n",
        "            B = F.pad(B, (int((1 + self.size[1] - B.size[0]) / 2), 0))\n",
        "        # pad the height if needed\n",
        "        if self.pad_if_needed and A.size[1] < self.size[0]:\n",
        "            A = F.pad(A, (0, int((1 + self.size[0] - A.size[1]) / 2)))\n",
        "            B = F.pad(B, (0, int((1 + self.size[0] - B.size[1]) / 2)))\n",
        "\n",
        "        i, j, h, w = self.get_params(A, self.size)\n",
        "        sample['A'] = F.crop(A, i, j, h, w)\n",
        "        sample['B'] = F.crop(B, i, j, h, w)\n",
        "\n",
        "        # _i, _j, _h, _w = self.get_params(A, self.size)\n",
        "        # sample['A'] = F.crop(A, i, j, h, w)\n",
        "        # sample['B'] = F.crop(B, _i, _j, _h, _w)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class CenterCrop(transforms.CenterCrop):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "        sample['A'] = F.center_crop(A, self.size)\n",
        "        sample['B'] = F.center_crop(B, self.size)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(transforms.RandomHorizontalFlip):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "        if random.random() > 0.5:\n",
        "            A = F.hflip(A)\n",
        "            B = F.hflip(B)\n",
        "\n",
        "        sample['A'] = A\n",
        "        sample['B'] = B\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def find_classes(dir):\n",
        "    \"\"\"\n",
        "    Finds the class folders in a dataset.\n",
        "\n",
        "    Args:\n",
        "        dir (string): Root directory path.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
        "\n",
        "    Ensures:\n",
        "        No class is a subdirectory of another.\n",
        "    \"\"\"\n",
        "    #在表达式 sys.version_info >= (3, 5) 中，它检查Python解释器的版本是否大于或等于 3.5。\n",
        "    if sys.version_info >= (3, 5):\n",
        "        # Faster and available in Python 3.5 and above\n",
        "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "    else:\n",
        "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "class Resize(transforms.Resize):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "        h = self.size[0]\n",
        "        w = self.size[1]\n",
        "\n",
        "        sample['A'] = F.resize(A, (h, w))\n",
        "        sample['B'] = F.resize(B, (h, w))\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "\n",
        "        # if isinstance(sample, dict):\n",
        "        #     for key, value in sample:\n",
        "        #         _list = sample[key]\n",
        "        #         sample[key] = [F.to_tensor(item) for item in _list]\n",
        "\n",
        "        sample['A'] = F.to_tensor(A)\n",
        "        sample['B'] = F.to_tensor(B)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Normalize(transforms.Normalize):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        A, B = sample['A'], sample['B']\n",
        "        sample['A'] = F.normalize(A, self.mean, self.std)\n",
        "        sample['B'] = F.normalize(B, self.mean, self.std)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Lambda(transforms.Lambda):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.lambd(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wahpIY6uhbP_"
      },
      "source": [
        "模型部分\n",
        "\n",
        "1.   基础模型\n",
        "2.   图像编码\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI5SJ_9ANM6V"
      },
      "source": [
        "#backbone是Res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv87AdoWhq6g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        model = torchvision.models.resnet18(pretrained=True)\n",
        "        #遍历函数的子层，这里即是features和classifier，然后把除去最后一层的层放到一个列表\n",
        "        modules = list(model.children())[:-1]\n",
        "        #变量前加一个星号*，目的是将该list变量拆解开多个独立的参数，传入函数中\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        pool_func = (\n",
        "            nn.AdaptiveAvgPool2d\n",
        "            if args.img_embed_pool_type == \"avg\"\n",
        "            else nn.AdaptiveMaxPool2d\n",
        "        )\n",
        "\n",
        "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
        "            self.pool = pool_func((args.num_image_embeds, 1))\n",
        "        elif args.num_image_embeds == 4:\n",
        "            self.pool = pool_func((2, 2))\n",
        "        elif args.num_image_embeds == 6:\n",
        "            self.pool = pool_func((3, 2))\n",
        "        elif args.num_image_embeds == 8:\n",
        "            self.pool = pool_func((4, 2))\n",
        "        elif args.num_image_embeds == 9:\n",
        "            self.pool = pool_func((3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
        "       out = self.model(x)\n",
        "       #print('model out',out.shape)\n",
        "       out = self.pool(out)\n",
        "       #print('pool out',out.shape)\n",
        "       out = torch.flatten(out, start_dim=2)\n",
        "       #print('flatten out',out.shape)\n",
        "       out = out.transpose(1, 2).contiguous()\n",
        "       #print('transpose out',out.shape)\n",
        "       return out  # BxNx2048\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUyijWAmLKEZ"
      },
      "source": [
        "#backbone更换为Mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6SlXDW1LGo_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from vision_mamba import Vim\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.args = args\n",
        "        #加载VisionMamba,这里输出类改为了和Res一致的args.img_hidden_sz，后续需要根据训练结果再看看\n",
        "        model = Vim(\n",
        "            dim=256,  # Dimension of the transformer model\n",
        "            dt_rank=32,  # Rank of the dynamic routing matrix\n",
        "            dim_inner=256,  # Inner dimension of the transformer model\n",
        "            d_state=256,  # Dimension of the state vector\n",
        "            num_classes=args.img_hidden_sz,  # Number of output classes\n",
        "            image_size=args.FINE_SIZE,  # Size of the input image\n",
        "            patch_size=16,  # Size of each image patch\n",
        "            channels=3,  # Number of input channels\n",
        "            dropout=0.1,  # Dropout rate\n",
        "            depth=12,  # Depth of the transformer model\n",
        "        )\n",
        "        #modules = list(model.children())\n",
        "        #变量前加一个星号*，目的是将该list变量拆解开多个独立的参数，传入函数中\n",
        "        self.model = model\n",
        "        #nn.AdaptiveAvgPool2d 对由多个输入平面组成的输入信号应用二维自适应平均池化。接受两个参数，分别为输出特征图的长和宽\n",
        "        pool_func = (\n",
        "            nn.AdaptiveAvgPool2d\n",
        "            if args.img_embed_pool_type == \"avg\"\n",
        "            else nn.AdaptiveMaxPool2d\n",
        "        )\n",
        "\n",
        "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
        "            self.pool = pool_func((args.num_image_embeds, 1))\n",
        "        elif args.num_image_embeds == 4:\n",
        "            self.pool = pool_func((2, 2))\n",
        "        elif args.num_image_embeds == 6:\n",
        "            self.pool = pool_func((3, 2))\n",
        "        elif args.num_image_embeds == 8:\n",
        "            self.pool = pool_func((4, 2))\n",
        "        elif args.num_image_embeds == 9:\n",
        "            self.pool = pool_func((3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
        "        out = self.model(x)\n",
        "        out = self.pool(out)\n",
        "        #torch.flatten 作用是改变张量的维度和维数，从指定的维度开始将后面维度的维数全部展成一个维度，新的维数就是被展开的所有维度的维数的乘积。是按照x的第2个维度拼接\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        #transpose交换out的第一个维度和第二个维度，\n",
        "        #调用contiguous()时，会强制拷贝一份tensor，让它的布局和从头创建的一模一样，但是两个tensor完全没有联系，\n",
        "        #更改其中一个的值不会影像到里一个。\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        return out  # BxNx2048"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzw-Dn2Y92Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#一般狄利克雷函数"
      ],
      "metadata": {
        "id": "LJQCWP6-8YGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_gIFAUuhoDP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# loss function\n",
        "def HD(alpha, c):\n",
        "    a = 2.0\n",
        "    b = a/(a-1)\n",
        "    beta = torch.ones((1, c)).cuda()\n",
        "\n",
        "    F1_1 = torch.sum(torch.lgamma(a * alpha + beta), dim=1, keepdim=True) - torch.lgamma(torch.sum((a * alpha + beta), dim=1, keepdim=True))\n",
        "    F2_1 = torch.sum(torch.lgamma((b + 1) * beta), dim=1, keepdim=True) - torch.lgamma(torch.sum(((b + 1) * beta), dim=1, keepdim=True))\n",
        "    F3_1 = torch.sum(torch.lgamma(alpha + 2 * beta), dim=1, keepdim=True) - torch.lgamma(torch.sum((alpha + 2 * beta), dim=1, keepdim=True))\n",
        "\n",
        "    hd1 = 1/a * F1_1 + 1/b * F2_1 - F3_1\n",
        "    print('f1:{},f2:{},f3:{},hd1:{}'.format(F1_1,F2_1,F3_1,hd1))\n",
        "    a = b\n",
        "    b = a/(a-1)\n",
        "    F1_2 = torch.sum(torch.lgamma(a * alpha + beta), dim=1, keepdim=True) - torch.lgamma(torch.sum((a * alpha + beta), dim=1, keepdim=True))\n",
        "    F2_2 = torch.sum(torch.lgamma((b + 1) * beta), dim=1, keepdim=True) - torch.lgamma(torch.sum(((b + 1) *  beta), dim=1, keepdim=True))\n",
        "    F3_2 = torch.sum(torch.lgamma(alpha + 2 * beta), dim=1, keepdim=True) - torch.lgamma(torch.sum((alpha + 2 * beta), dim=1, keepdim=True))\n",
        "\n",
        "    hd2 = 1/a * F1_2 + 1/b * F2_2 - F3_2\n",
        "\n",
        "    hd = 1/2 * (hd1+hd2)\n",
        "    return hd\n",
        "\n",
        "def ce_loss(p, alpha, c, global_step, annealing_step):\n",
        "    #print(alpha)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    E = alpha - 1\n",
        "    #将标签转成one-hot编码\n",
        "    label = F.one_hot(p, num_classes=c)\n",
        "    #print(\"Label shape:\", label.shape)\n",
        "    #print(\"S shape:\", S.shape)\n",
        "    #print(\"Alpha shape:\", alpha.shape)\n",
        "    A = torch.sum(label * (torch.digamma(S) - torch.digamma(alpha)), dim=1, keepdim=True)\n",
        "\n",
        "    annealing_coef = min(1, global_step / annealing_step)\n",
        "    alp = E * (1 - label) + 1\n",
        "    B = annealing_coef * HD(alp, c)\n",
        "    #print('A:{},HD:{}'.format(A,B))\n",
        "    print('A+B:{}'.format(torch.mean((A + B))))\n",
        "    return torch.mean((A + B))\n",
        "\n",
        "\n",
        "class TMC(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TMC, self).__init__()\n",
        "        self.args = args\n",
        "        #获得模型初始化参数，其中池化层是自定义的\n",
        "        self.rgbenc = ImageEncoder(args)\n",
        "        self.depthenc = ImageEncoder(args)\n",
        "        depth_last_size = args.img_hidden_sz * args.num_image_embeds\n",
        "        rgb_last_size = args.img_hidden_sz * args.num_image_embeds\n",
        "        #nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。\n",
        "        self.clf_depth = nn.ModuleList()\n",
        "        self.clf_rgb = nn.ModuleList()\n",
        "        for hidden in args.hidden:\n",
        "            self.clf_depth.append(nn.Linear(depth_last_size, hidden))\n",
        "            self.clf_depth.append(nn.ReLU())\n",
        "            self.clf_depth.append(nn.Dropout(args.dropout))\n",
        "            depth_last_size = hidden\n",
        "        self.clf_depth.append(nn.Linear(depth_last_size, args.n_classes))\n",
        "\n",
        "        for hidden in args.hidden:\n",
        "            self.clf_rgb.append(nn.Linear(rgb_last_size, hidden))\n",
        "            self.clf_rgb.append(nn.ReLU())\n",
        "            self.clf_rgb.append(nn.Dropout(args.dropout))\n",
        "            rgb_last_size = hidden\n",
        "        self.clf_rgb.append(nn.Linear(rgb_last_size, args.n_classes))\n",
        "\n",
        "    def DS_Combin_two(self, alpha1, alpha2):\n",
        "        # Calculate the merger of two DS evidences\n",
        "        alpha = dict()\n",
        "        alpha[0], alpha[1] = alpha1, alpha2\n",
        "        b, S, E, u = dict(), dict(), dict(), dict()\n",
        "        for v in range(2):\n",
        "            #计算所有参数的和然后保留这个维度\n",
        "            S[v] = torch.sum(alpha[v], dim=1, keepdim=True)\n",
        "            E[v] = alpha[v] - 1\n",
        "            #将单个维度扩大成更大维度，返回一个新的tensor\n",
        "            b[v] = E[v] / (S[v].expand(E[v].shape))\n",
        "            u[v] = self.args.n_classes / S[v]\n",
        "\n",
        "        # b^0 @ b^(0+1)   torch.bmm计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,m)\n",
        "        #也就是说两个tensor的第一维是相等的，然后第一个数组的第三维和第二个数组的第二维度要求一样\n",
        "        bb = torch.bmm(b[0].view(-1, self.args.n_classes, 1), b[1].view(-1, 1, self.args.n_classes))\n",
        "        # b^0 * u^1\n",
        "        #torch.mul是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵\n",
        "        uv1_expand = u[1].expand(b[0].shape)\n",
        "        bu = torch.mul(b[0], uv1_expand)\n",
        "        # b^1 * u^0\n",
        "        uv_expand = u[0].expand(b[0].shape)\n",
        "        ub = torch.mul(b[1], uv_expand)\n",
        "        # calculate K\n",
        "        bb_sum = torch.sum(bb, dim=(1, 2), out=None)  #计算bb内所有参数的和\n",
        "        bb_diag = torch.diagonal(bb, dim1=-2, dim2=-1).sum(-1)  #求bb内对角线附件的元素，并将最后一个维度求和\n",
        "        # bb_diag1 = torch.diag(torch.mm(b[v], torch.transpose(b[v+1], 0, 1)))\n",
        "        K = bb_sum - bb_diag\n",
        "\n",
        "        # calculate b^a\n",
        "        b_a = (torch.mul(b[0], b[1]) + bu + ub) / ((1 - K).view(-1, 1).expand(b[0].shape))\n",
        "        # calculate u^a\n",
        "        u_a = torch.mul(u[0], u[1]) / ((1 - K).view(-1, 1).expand(u[0].shape))\n",
        "        # test = torch.sum(b_a, dim = 1, keepdim = True) + u_a #Verify programming errors\n",
        "\n",
        "        # calculate new S\n",
        "        S_a = self.args.n_classes / u_a\n",
        "        # calculate new e_k\n",
        "        e_a = torch.mul(b_a, S_a.expand(b_a.shape))\n",
        "        alpha_a = e_a + 1\n",
        "        return alpha_a\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        #首先输入depth数据到resnet18分类网络中，然后将分类结果作为预训练的结果扁平化处理\n",
        "        depth = self.depthenc(depth)\n",
        "        depth = torch.flatten(depth, start_dim=1)\n",
        "        #首先输入rgb数据到resnet18分类网络中，然后将分类结果作为预训练的结果扁平化处理\n",
        "        rgb = self.rgbenc(rgb)\n",
        "        rgb = torch.flatten(rgb, start_dim=1)\n",
        "\n",
        "        #对通过模型取得分类结果\n",
        "        depth_out = depth\n",
        "        for layer in self.clf_depth:\n",
        "            depth_out = layer(depth_out)\n",
        "        rgb_out = rgb\n",
        "        for layer in self.clf_rgb:\n",
        "            rgb_out = layer(rgb_out)\n",
        "\n",
        "        #Softplus函数可以看作是ReLU激活函数的平滑代替。\n",
        "        depth_evidence, rgb_evidence = F.softplus(depth_out), F.softplus(rgb_out)\n",
        "        depth_alpha, rgb_alpha = depth_evidence+1, rgb_evidence+1\n",
        "        depth_rgb_alpha = self.DS_Combin_two(depth_alpha, rgb_alpha)\n",
        "        return depth_alpha, rgb_alpha, depth_rgb_alpha\n",
        "\n",
        "\n",
        "class ETMC(TMC):\n",
        "    def __init__(self, args):\n",
        "        super(ETMC, self).__init__(args)\n",
        "        #这一部分需要研究一下\n",
        "        last_size = args.img_hidden_sz * args.num_image_embeds + args.img_hidden_sz * args.num_image_embeds\n",
        "        self.clf = nn.ModuleList()\n",
        "        for hidden in args.hidden:\n",
        "            self.clf.append(nn.Linear(last_size, hidden))\n",
        "            self.clf.append(nn.ReLU())\n",
        "            self.clf.append(nn.Dropout(args.dropout))\n",
        "            last_size = hidden\n",
        "        self.clf.append(nn.Linear(last_size, args.n_classes))\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "       # print(\"depth shape:\", depth.shape)\n",
        "        depth = self.depthenc(depth)\n",
        "       # print(\"depthenc shape:\", depth.shape)\n",
        "        depth = torch.flatten(depth, start_dim=1)\n",
        "        #print(\"flatted depth shape:\", depth.shape)\n",
        "        rgb = self.rgbenc(rgb)\n",
        "        rgb = torch.flatten(rgb, start_dim=1)\n",
        "        depth_out = depth\n",
        "        for layer in self.clf_depth:\n",
        "            depth_out = layer(depth_out)\n",
        "        #print(\"depth_out shape:\", depth_out.shape)\n",
        "        rgb_out = rgb\n",
        "        for layer in self.clf_rgb:\n",
        "            rgb_out = layer(rgb_out)\n",
        "\n",
        "        #按列拼接数据\n",
        "        pseudo_out = torch.cat([rgb, depth], -1)\n",
        "        for layer in self.clf:\n",
        "            pseudo_out = layer(pseudo_out)\n",
        "\n",
        "        depth_evidence, rgb_evidence, pseudo_evidence = F.softplus(depth_out), F.softplus(rgb_out), F.softplus(pseudo_out)\n",
        "        #print(\"depth_evidence shape:\", depth_evidence.shape)\n",
        "        depth_alpha, rgb_alpha, pseudo_alpha = depth_evidence+1, rgb_evidence+1, pseudo_evidence+1\n",
        "        depth_rgb_alpha = self.DS_Combin_two(self.DS_Combin_two(depth_alpha, rgb_alpha), pseudo_alpha)\n",
        "        return depth_alpha, rgb_alpha, pseudo_alpha, depth_rgb_alpha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#光滑狄利克雷函数"
      ],
      "metadata": {
        "id": "3w_-geBf8kCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# loss function\n",
        "def HD(alpha, c):\n",
        "    a = 2.0\n",
        "    b = a/(a-1)\n",
        "    beta = torch.ones((1, c)).cuda()\n",
        "\n",
        "    F1_1 = torch.lgamma(torch.sum(a * (alpha - 1) + 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(a * (alpha - 1) + 1), dim=1, keepdim=True)\n",
        "    F2_1 = torch.lgamma(torch.sum(b * (beta - 1) + 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(b * (beta - 1) + 1), dim=1, keepdim=True)\n",
        "    F3_1 = torch.lgamma(torch.sum(alpha + beta - 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(alpha + beta - 1), dim=1, keepdim=True)\n",
        "    # Combine the terms into the final result\n",
        "    hd1 = (1 / a) * F1_1 + (1 / b) * F2_1 - F3_1\n",
        "    #print('f1:{},f2:{},f3:{},hd1:{}'.format(F1_1,F2_1,F3_1,hd1))\n",
        "    a = b\n",
        "    b = a/(a-1)\n",
        "    F1_2 = torch.lgamma(torch.sum(a * (alpha - 1) + 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(a * (alpha - 1) + 1), dim=1, keepdim=True)\n",
        "    F2_2 = torch.lgamma(torch.sum(b * (beta - 1) + 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(b * (beta - 1) + 1), dim=1, keepdim=True)\n",
        "    F3_2 = torch.lgamma(torch.sum(alpha + beta - 1, dim=1, keepdim=True)) - torch.sum(torch.lgamma(alpha + beta - 1), dim=1, keepdim=True)\n",
        "\n",
        "    hd2 = (1 / a) * F1_2 + (1 / b) * F2_2 - F3_2\n",
        "\n",
        "    hd = -1/2 * (hd1+hd2)\n",
        "    #hd = hd1\n",
        "    return hd\n",
        "\n",
        "def ce_loss(p, alpha, c, global_step, annealing_step):\n",
        "    #print(alpha)\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    E = alpha - 1\n",
        "    #将标签转成one-hot编码\n",
        "    label = F.one_hot(p, num_classes=c)\n",
        "    #print(\"Label shape:\", label.shape)\n",
        "    #print(\"S shape:\", S.shape)\n",
        "    #print(\"Alpha shape:\", alpha.shape)\n",
        "    A = torch.sum(label * (torch.digamma(S) - torch.digamma(alpha)), dim=1, keepdim=True)\n",
        "\n",
        "    annealing_coef = min(1, global_step / annealing_step)\n",
        "    alp = E * (1 - label) + 1\n",
        "    B = annealing_coef * HD(alp, c)\n",
        "    return torch.mean((A + B))\n",
        "\n",
        "\n",
        "class TMC(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TMC, self).__init__()\n",
        "        self.args = args\n",
        "        #获得模型初始化参数，其中池化层是自定义的\n",
        "        self.rgbenc = ImageEncoder(args)\n",
        "        self.depthenc = ImageEncoder(args)\n",
        "        depth_last_size = args.img_hidden_sz * args.num_image_embeds\n",
        "        rgb_last_size = args.img_hidden_sz * args.num_image_embeds\n",
        "        #nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。\n",
        "        self.clf_depth = nn.ModuleList()\n",
        "        self.clf_rgb = nn.ModuleList()\n",
        "        for hidden in args.hidden:\n",
        "            self.clf_depth.append(nn.Linear(depth_last_size, hidden))\n",
        "            self.clf_depth.append(nn.ReLU())\n",
        "            self.clf_depth.append(nn.Dropout(args.dropout))\n",
        "            depth_last_size = hidden\n",
        "        self.clf_depth.append(nn.Linear(depth_last_size, args.n_classes))\n",
        "\n",
        "        for hidden in args.hidden:\n",
        "            self.clf_rgb.append(nn.Linear(rgb_last_size, hidden))\n",
        "            self.clf_rgb.append(nn.ReLU())\n",
        "            self.clf_rgb.append(nn.Dropout(args.dropout))\n",
        "            rgb_last_size = hidden\n",
        "        self.clf_rgb.append(nn.Linear(rgb_last_size, args.n_classes))\n",
        "\n",
        "    def DS_Combin_two(self, alpha1, alpha2):\n",
        "        # Calculate the merger of two DS evidences\n",
        "        alpha = dict()\n",
        "        alpha[0], alpha[1] = alpha1, alpha2\n",
        "        b, S, E, u = dict(), dict(), dict(), dict()\n",
        "        for v in range(2):\n",
        "            #计算所有参数的和然后保留这个维度\n",
        "            S[v] = torch.sum(alpha[v], dim=1, keepdim=True)\n",
        "            E[v] = alpha[v] - 1\n",
        "            #将单个维度扩大成更大维度，返回一个新的tensor\n",
        "            b[v] = E[v] / (S[v].expand(E[v].shape))\n",
        "            u[v] = self.args.n_classes / S[v]\n",
        "\n",
        "        # b^0 @ b^(0+1)   torch.bmm计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,m)\n",
        "        #也就是说两个tensor的第一维是相等的，然后第一个数组的第三维和第二个数组的第二维度要求一样\n",
        "        bb = torch.bmm(b[0].view(-1, self.args.n_classes, 1), b[1].view(-1, 1, self.args.n_classes))\n",
        "        # b^0 * u^1\n",
        "        #torch.mul是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵\n",
        "        uv1_expand = u[1].expand(b[0].shape)\n",
        "        bu = torch.mul(b[0], uv1_expand)\n",
        "        # b^1 * u^0\n",
        "        uv_expand = u[0].expand(b[0].shape)\n",
        "        ub = torch.mul(b[1], uv_expand)\n",
        "        # calculate K\n",
        "        bb_sum = torch.sum(bb, dim=(1, 2), out=None)  #计算bb内所有参数的和\n",
        "        bb_diag = torch.diagonal(bb, dim1=-2, dim2=-1).sum(-1)  #求bb内对角线附件的元素，并将最后一个维度求和\n",
        "        # bb_diag1 = torch.diag(torch.mm(b[v], torch.transpose(b[v+1], 0, 1)))\n",
        "        K = bb_sum - bb_diag\n",
        "\n",
        "        # calculate b^a\n",
        "        b_a = (torch.mul(b[0], b[1]) + bu + ub) / ((1 - K).view(-1, 1).expand(b[0].shape))\n",
        "        # calculate u^a\n",
        "        u_a = torch.mul(u[0], u[1]) / ((1 - K).view(-1, 1).expand(u[0].shape))\n",
        "        # test = torch.sum(b_a, dim = 1, keepdim = True) + u_a #Verify programming errors\n",
        "\n",
        "        # calculate new S\n",
        "        S_a = self.args.n_classes / u_a\n",
        "        # calculate new e_k\n",
        "        e_a = torch.mul(b_a, S_a.expand(b_a.shape))\n",
        "        alpha_a = e_a + 1\n",
        "        return alpha_a\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        #首先输入depth数据到resnet18分类网络中，然后将分类结果作为预训练的结果扁平化处理\n",
        "        depth = self.depthenc(depth)\n",
        "        depth = torch.flatten(depth, start_dim=1)\n",
        "        #首先输入rgb数据到resnet18分类网络中，然后将分类结果作为预训练的结果扁平化处理\n",
        "        rgb = self.rgbenc(rgb)\n",
        "        rgb = torch.flatten(rgb, start_dim=1)\n",
        "\n",
        "        #对通过模型取得分类结果\n",
        "        depth_out = depth\n",
        "        for layer in self.clf_depth:\n",
        "            depth_out = layer(depth_out)\n",
        "        rgb_out = rgb\n",
        "        for layer in self.clf_rgb:\n",
        "            rgb_out = layer(rgb_out)\n",
        "\n",
        "        #Softplus函数可以看作是ReLU激活函数的平滑代替。\n",
        "        depth_evidence, rgb_evidence = F.softplus(depth_out), F.softplus(rgb_out)\n",
        "        depth_alpha, rgb_alpha = depth_evidence+1, rgb_evidence+1\n",
        "        depth_rgb_alpha = self.DS_Combin_two(depth_alpha, rgb_alpha)\n",
        "        return depth_alpha, rgb_alpha, depth_rgb_alpha\n",
        "\n",
        "\n",
        "class ETMC(TMC):\n",
        "    def __init__(self, args):\n",
        "        super(ETMC, self).__init__(args)\n",
        "        #这一部分需要研究一下\n",
        "        last_size = args.img_hidden_sz * args.num_image_embeds + args.img_hidden_sz * args.num_image_embeds\n",
        "        self.clf = nn.ModuleList()\n",
        "        for hidden in args.hidden:\n",
        "            self.clf.append(nn.Linear(last_size, hidden))\n",
        "            self.clf.append(nn.ReLU())\n",
        "            self.clf.append(nn.Dropout(args.dropout))\n",
        "            last_size = hidden\n",
        "        self.clf.append(nn.Linear(last_size, args.n_classes))\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "       # print(\"depth shape:\", depth.shape)\n",
        "        depth = self.depthenc(depth)\n",
        "       # print(\"depthenc shape:\", depth.shape)\n",
        "        depth = torch.flatten(depth, start_dim=1)\n",
        "        #print(\"flatted depth shape:\", depth.shape)\n",
        "        rgb = self.rgbenc(rgb)\n",
        "        rgb = torch.flatten(rgb, start_dim=1)\n",
        "        depth_out = depth\n",
        "        for layer in self.clf_depth:\n",
        "            depth_out = layer(depth_out)\n",
        "        #print(\"depth_out shape:\", depth_out.shape)\n",
        "        rgb_out = rgb\n",
        "        for layer in self.clf_rgb:\n",
        "            rgb_out = layer(rgb_out)\n",
        "\n",
        "        #按列拼接数据\n",
        "        pseudo_out = torch.cat([rgb, depth], -1)\n",
        "        for layer in self.clf:\n",
        "            pseudo_out = layer(pseudo_out)\n",
        "\n",
        "        depth_evidence, rgb_evidence, pseudo_evidence = F.softplus(depth_out), F.softplus(rgb_out), F.softplus(pseudo_out)\n",
        "        #print(\"depth_evidence shape:\", depth_evidence.shape)\n",
        "        depth_alpha, rgb_alpha, pseudo_alpha = depth_evidence+1, rgb_evidence+1, pseudo_evidence+1\n",
        "        depth_rgb_alpha = self.DS_Combin_two(self.DS_Combin_two(depth_alpha, rgb_alpha), pseudo_alpha)\n",
        "        return depth_alpha, rgb_alpha, pseudo_alpha, depth_rgb_alpha\n"
      ],
      "metadata": {
        "id": "MHEBeK3PrH-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2lTSCI8h9X-"
      },
      "source": [
        "#工具函数 utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRjkkKCXiFh_"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "class LogFormatter:\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    #round()用于数字的四舍五入。\n",
        "    def format(self, record):\n",
        "        #获取程序运行的时间\n",
        "        elapsed_seconds = round(record.created - self.start_time)\n",
        "        #levelname代表日志的输出级别，包括 ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n",
        "        #time.strftime(\"%x %X\")按年月日时间的方式输出\n",
        "        #timedelta() 表示两个 date 对象或者 time 对象,或者 datetime 对象之间的时间间隔，精确到微秒。\n",
        "        prefix = \"%s - %s - %s\" % (\n",
        "            record.levelname,\n",
        "            time.strftime(\"%x %X\"),\n",
        "            timedelta(seconds=elapsed_seconds),\n",
        "        )\n",
        "        #GetMessage的主要功能是从消息队列中“取出”消息，消息被取出以后，就从消息队列中将其删除；\n",
        "        message = record.getMessage()\n",
        "        message = message.replace(\"\\n\", \"\\n\" + \" \" * (len(prefix) + 3))\n",
        "        return \"%s - %s\" % (prefix, message)\n",
        "\n",
        "\n",
        "def create_logger(filepath, args):\n",
        "    # create log formatter,创建输出日志的格式\n",
        "    log_formatter = LogFormatter()\n",
        "\n",
        "    # create file handler and set level to debug\n",
        "    file_handler = logging.FileHandler(filepath, \"a\")\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "    file_handler.setFormatter(log_formatter)\n",
        "\n",
        "    # create console handler and set level to info\n",
        "    #Handler对象负责将适当的日志消息（基于日志消息的严重性）分派到处理程序的指定目标。\n",
        "    #ch=logging.StreamHandler() # 将日志写入控制台\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "\n",
        "    # create logger and set level to debug\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    #确保当前记录器进行记录时则不会调用上层记录器\n",
        "    logger.propagate = False\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    # reset logger elapsed time\n",
        "    def reset_time():\n",
        "        log_formatter.start_time = time.time()\n",
        "\n",
        "    logger.reset_time = reset_time\n",
        "    #利用sorted函数对可迭代对象进行排序，按照元素items的标签进行升序\n",
        "    logger.info(\n",
        "        \"\\n\".join(\n",
        "            \"%s: %s\" % (k, str(v))\n",
        "            for k, v in sorted(dict(vars(args)).items(), key=lambda x: x[0])\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulQzifhDh9DN"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "#默认seed=1\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    #生成随机数组\n",
        "    np.random.seed(seed)\n",
        "    #设置CPU生成随机数的种子，方便下次复现实验结果\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    ##为当前GPU设置随机种子；\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    #每次返回的卷积算法将是确定的\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    #设置为True，会使得cuDNN来衡量自己库里面的多个卷积算法的速度，然后选择其中最快的那个卷积算法\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
        "    filename = os.path.join(checkpoint_path, filename)\n",
        "    torch.save(state, filename)\n",
        "    #shutil.copyfile(filename, dst):将名为filename的文件的内容（无元数据）复制到名为dst的文件中.dst必须是完整的目标文件名\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
        "\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    best_checkpoint = torch.load(path)\n",
        "    #给模型对象加载训练好的模型参数，即加载模型参数\n",
        "    model.load_state_dict(best_checkpoint[\"state_dict\"])\n",
        "\n",
        "#输出日志信息，包含训练一次对应的损失函数值，深度视角准确度，rgb色彩准确度\n",
        "#logger.info 是消息级别，当输出这个级别的消息时，会输出后面规定格式的文件\n",
        "def log_metrics(set_name, metrics, logger):\n",
        "    logger.info(\n",
        "        \"{}: Loss: {:.5f} | depth_acc: {:.5f}, rgb_acc: {:.5f}\".format(\n",
        "            set_name, metrics[\"loss\"], metrics[\"depth_acc\"], metrics[\"rgb_acc\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def numpy_seed(seed, *addl_seeds):\n",
        "    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n",
        "    restores the state afterward\"\"\"\n",
        "    #人为设置一个断点，可以将yield看成一个断点，程序在断点处结束运行，又从断点处开始运行\n",
        "    if seed is None:\n",
        "        yield\n",
        "        return\n",
        "    #返回数据的哈希值，并除以10的六次方\n",
        "    if len(addl_seeds) > 0:\n",
        "        seed = int(hash((seed, *addl_seeds)) % 1e6)\n",
        "    #获取随机生成器 np.random的状态 ，一次来确保哈希生成器每次工作的值的是一样的\n",
        "    state = np.random.get_state()\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        np.random.set_state(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrbSM3T8iUAY"
      },
      "source": [
        "#分别用ETMC训练 nyud2,先设置参数"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_args(parser):\n",
        "    parser.add_argument(\"--batch_sz\", type=int, default=32)\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"/content/gdrive/My Drive/ETMC/datasets/nyud2/\")\n",
        "    parser.add_argument(\"--LOAD_SIZE\", type=int, default=256)\n",
        "    parser.add_argument(\"--FINE_SIZE\", type=int, default=224)\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=3)\n",
        "    parser.add_argument(\"--hidden\", nargs=\"*\", type=int, default=[])\n",
        "    parser.add_argument(\"--hidden_sz\", type=int, default=768)\n",
        "    parser.add_argument(\"--img_embed_pool_type\", type=str, default=\"avg\", choices=[\"max\", \"avg\"])\n",
        "    parser.add_argument(\"--img_hidden_sz\", type=int, default=512)\n",
        "    parser.add_argument(\"--include_bn\", type=int, default=True)\n",
        "    parser.add_argument(\"--lr\", type=float, default=5e-4)\n",
        "    parser.add_argument(\"--lr_factor\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--lr_patience\", type=int, default=30)\n",
        "    parser.add_argument(\"--max_epochs\", type=int, default=500)\n",
        "    parser.add_argument(\"--n_workers\", type=int, default=12)\n",
        "    parser.add_argument(\"--name\", type=str, default=\"resReleasedVersion\")\n",
        "    parser.add_argument(\"--num_image_embeds\", type=int, default=1)\n",
        "    parser.add_argument(\"--patience\", type=int, default=30)\n",
        "    parser.add_argument(\"--savedir\", type=str, default=\"/content/gdrive/My Drive/ETMC/results/nyud2/\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=1)\n",
        "    parser.add_argument(\"--n_classes\", type=int, default=10)\n",
        "    parser.add_argument(\"--annealing_epoch\", type=int, default=10)\n",
        "    parser.add_argument(\"--Holder_parge\", type=float, default=1.5)\n",
        "    parser.add_argument(\"--backbone\", type=str, default='Res')\n",
        "def get_optimizer(model, args):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, args):\n",
        "    return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", patience=args.lr_patience, verbose=True, factor=args.lr_factor\n",
        "    )\n",
        "\n",
        "\n",
        "def model_forward(i_epoch, model, args, ce_loss, batch):\n",
        "    rgb, depth, tgt = batch['A'], batch['B'], batch['label']\n",
        "\n",
        "\n",
        "    rgb, depth, tgt = rgb.cuda(), depth.cuda(), tgt.cuda()\n",
        "    depth_alpha, rgb_alpha, pseudo_alpha, depth_rgb_alpha = model(rgb, depth)\n",
        "\n",
        "    loss = ce_loss(tgt, depth_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
        "           ce_loss(tgt, rgb_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
        "           ce_loss(tgt, pseudo_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
        "           ce_loss(tgt, depth_rgb_alpha, args.n_classes, i_epoch, args.annealing_epoch)\n",
        "    return loss, depth_alpha, rgb_alpha, depth_rgb_alpha, tgt\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, args, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        losses, depth_preds, rgb_preds, depthrgb_preds, tgts = [], [], [], [], []\n",
        "        for batch in data:\n",
        "            loss, depth_alpha, rgb_alpha, depth_rgb_alpha, tgt = model_forward(i_epoch, model, args, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            depth_pred = depth_alpha.argmax(dim=1).cpu().detach().numpy()\n",
        "            rgb_pred = rgb_alpha.argmax(dim=1).cpu().detach().numpy()\n",
        "            depth_rgb_pred = depth_rgb_alpha.argmax(dim=1).cpu().detach().numpy()\n",
        "\n",
        "            depth_preds.append(depth_pred)\n",
        "            rgb_preds.append(rgb_pred)\n",
        "            depthrgb_preds.append(depth_rgb_pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    depth_preds = [l for sl in depth_preds for l in sl]\n",
        "    rgb_preds = [l for sl in rgb_preds for l in sl]\n",
        "    depthrgb_preds = [l for sl in depthrgb_preds for l in sl]\n",
        "    metrics[\"depth_acc\"] = accuracy_score(tgts, depth_preds)\n",
        "    metrics[\"rgb_acc\"] = accuracy_score(tgts, rgb_preds)\n",
        "    metrics[\"depthrgb_acc\"] = accuracy_score(tgts, depthrgb_preds)\n",
        "    metrics[\"depth_pre\"] = precision_score(tgts, depth_preds,average='weighted')\n",
        "    metrics[\"rgb_pre\"] = precision_score(tgts, rgb_preds,average='weighted')\n",
        "    metrics[\"depthrgb_pre\"] = precision_score(tgts, depthrgb_preds,average='weighted')\n",
        "    metrics[\"depth_rec\"] = recall_score(tgts, depth_preds,average='weighted')\n",
        "    metrics[\"rgb_rec\"] = recall_score(tgts, rgb_preds,average='weighted')\n",
        "    metrics[\"depthrgb_rec\"] = recall_score(tgts, depthrgb_preds,average='weighted')\n",
        "    metrics[\"depth_f1\"] = f1_score(tgts, depth_preds,average='weighted')\n",
        "    metrics[\"rgb_f1\"] = f1_score(tgts, rgb_preds,average='weighted')\n",
        "    metrics[\"depthrgb_f1\"] = f1_score(tgts, depthrgb_preds,average='weighted')\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    set_seed(args.seed)\n",
        "    args.savedir = os.path.join(args.savedir, args.name)\n",
        "    os.makedirs(args.savedir, exist_ok=True)\n",
        "    train_transforms = list()\n",
        "    train_transforms.append(transforms.Resize((args.LOAD_SIZE, args.LOAD_SIZE)))\n",
        "    train_transforms.append(transforms.RandomCrop((args.FINE_SIZE, args.FINE_SIZE)))\n",
        "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
        "    train_transforms.append(transforms.ToTensor())\n",
        "    train_transforms.append(transforms.Normalize(mean=[0.6983, 0.3918, 0.4474], std=[0.1648, 0.1359, 0.1644]))\n",
        "    val_transforms = list()\n",
        "    val_transforms.append(transforms.Resize((args.FINE_SIZE, args.FINE_SIZE)))\n",
        "    val_transforms.append(transforms.ToTensor())\n",
        "    val_transforms.append(transforms.Normalize(mean=[0.6983, 0.3918, 0.4474], std=[0.1648, 0.1359, 0.1644]))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        AlignedConcDataset(args, data_dir=os.path.join(args.data_path, 'train'), transform=transforms.Compose(train_transforms)),\n",
        "        batch_size=args.batch_sz,\n",
        "        shuffle=True,\n",
        "        num_workers=args.n_workers)\n",
        "    test_loader = DataLoader(\n",
        "            AlignedConcDataset(args, data_dir=os.path.join(args.data_path, 'test'), transform=transforms.Compose(val_transforms)),\n",
        "            batch_size=args.batch_sz,\n",
        "            shuffle=False,\n",
        "            num_workers=args.n_workers)\n",
        "    model = ETMC(args)\n",
        "    optimizer = get_optimizer(model, args)\n",
        "    scheduler = get_scheduler(optimizer, args)\n",
        "    logger = create_logger(\"%s/logfile.log\" % args.savedir, args)\n",
        "    model.cuda()\n",
        "\n",
        "    torch.save(args, os.path.join(args.savedir, \"args.pt\"))\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    #创建结果的csv格式输出文件\n",
        "    #dataset = \"nyudrgbd\"\n",
        "    filepath = 'results/HD_ETMC/nyud2_1_{}_{}_{}.csv'.format(args.backbone,args.Holder_parge,args.data_path.split('/')[-2])\n",
        "    #以新建的方式（会对原有文件进行覆盖），创建文件\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    f = open(filepath, \"w\")\n",
        "    f.writelines(\"loss,depth_acc,rgb_acc,depth_rgb_acc\\n\")\n",
        "\n",
        "    if os.path.exists(os.path.join(args.savedir, \"checkpoint.pt\")):\n",
        "        checkpoint = torch.load(os.path.join(args.savedir, \"checkpoint.pt\"))\n",
        "        start_epoch = checkpoint[\"epoch\"]\n",
        "        n_no_improve = checkpoint[\"n_no_improve\"]\n",
        "        best_metric = checkpoint[\"best_metric\"]\n",
        "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "\n",
        "    for i_epoch in range(start_epoch, args.max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        for batch in tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, depth_out, rgb_out, depthrgb, tgt = model_forward(i_epoch, model, args, ce_loss, batch)\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                 loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(\n",
        "            np.inf, test_loader, model, args, ce_loss\n",
        "        )\n",
        "        logger.info(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        # log_metrics(\"val\", metrics, logger)\n",
        "        # logger.info(\n",
        "        #     \"{}: Loss: {:.5f} | depth_acc: {:.5f}, rgb_acc: {:.5f}, depth rgb acc: {:.5f}\".format(\n",
        "        #         \"val\", metrics[\"loss\"], metrics[\"depth_acc\"], metrics[\"rgb_acc\"], metrics[\"depthrgb_acc\"]\n",
        "        #     )\n",
        "        # )\n",
        "        metric_types = [\"acc\", \"pre\", \"rec\", \"f1\"]\n",
        "        metric_prefixes = [\"depth\", \"rgb\", \"depthrgb\"]\n",
        "\n",
        "        for metric_type in metric_types:\n",
        "            metric_values = [metrics[f\"{prefix}_{metric_type}\"] for prefix in metric_prefixes]\n",
        "            log_message = \"{}: Loss: {:.5f} | {}: {:.5f}, {}: {:.5f}, {}: {:.5f}\".format(\n",
        "                \"Test\", metrics[\"loss\"],\n",
        "                f\"depth_{metric_type}\", metric_values[0],\n",
        "                f\"rgb_{metric_type}\", metric_values[1],\n",
        "                f\"depthrgb_{metric_type}\", metric_values[2]\n",
        "            )\n",
        "            logger.info(log_message)\n",
        "        tuning_metric = metrics[\"depthrgb_acc\"]\n",
        "\n",
        "        #向csv格式文件写入数据\n",
        "        f.writelines(f\"{metrics['loss']},{metrics['depth_acc']},{metrics['rgb_acc']},{metrics['depthrgb_acc']}\\n\")\n",
        "\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "\n",
        "        save_checkpoint(\n",
        "            {\n",
        "                \"epoch\": i_epoch + 1,\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "                \"n_no_improve\": n_no_improve,\n",
        "                \"best_metric\": best_metric,\n",
        "            },\n",
        "            is_improvement,\n",
        "            args.savedir,\n",
        "        )\n",
        "\n",
        "        if n_no_improve >= args.patience:\n",
        "            logger.info(\"No improvement. Breaking out of loop.\")\n",
        "            break\n",
        "\n",
        "    load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "    model.eval()\n",
        "    test_metrics = model_eval(\n",
        "        np.inf, test_loader, model, args, ce_loss\n",
        "    )\n",
        "    logger.info(\n",
        "        \"{}: Loss: {:.5f} | depth_acc: {:.5f}, rgb_acc: {:.5f}, depth rgb acc: {:.5f}\".format(\n",
        "            \"Test\", test_metrics[\"loss\"], test_metrics[\"depth_acc\"], test_metrics[\"rgb_acc\"],\n",
        "            test_metrics[\"depthrgb_acc\"]\n",
        "        )\n",
        "    )\n",
        "    logger.info(\n",
        "        \"{}: Loss: {:.5f} | depth_pre: {:.5f}, rgb_pre: {:.5f}, depth rgb pre: {:.5f}\".format(\n",
        "            \"Test\", test_metrics[\"loss\"], test_metrics[\"depth_pre\"], test_metrics[\"rgb_pre\"],\n",
        "            test_metrics[\"depthrgb_pre\"]\n",
        "        )\n",
        "    )\n",
        "    logger.info(\n",
        "        \"{}: Loss: {:.5f} | depth_rec: {:.5f}, rgb_rec: {:.5f}, depth rgb rec: {:.5f}\".format(\n",
        "            \"Test\", test_metrics[\"loss\"], test_metrics[\"depth_rec\"], test_metrics[\"rgb_rec\"],\n",
        "            test_metrics[\"depthrgb_rec\"]\n",
        "        )\n",
        "    )\n",
        "    logger.info(\n",
        "        \"{}: Loss: {:.5f} | depth_f1: {:.5f}, rgb_f1: {:.5f}, depth rgb f1: {:.5f}\".format(\n",
        "            \"Test\", test_metrics[\"loss\"], test_metrics[\"depth_f1\"], test_metrics[\"rgb_f1\"],\n",
        "            test_metrics[\"depthrgb_f1\"]\n",
        "        )\n",
        "    )\n",
        "    f.writelines(\"loss,best_depth_acc,best_rgb_acc,best_depth_rgb_acc\\n\")\n",
        "    f.writelines(f\"{test_metrics['loss']},{test_metrics['depth_acc']},{test_metrics['rgb_acc']},{test_metrics['depthrgb_acc']}\\n\")\n",
        "    f.close()\n",
        "    log_metrics(f\"Test\", test_metrics, logger)\n",
        "\n",
        "\n",
        "def cli_main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train Models\")\n",
        "    get_args(parser)\n",
        "    args =parser.parse_known_args()[0]\n",
        "    print(args)\n",
        "    #args, remaining_args = parser.parse_known_args()\n",
        "    #print(remaining_args)\n",
        "    #assert remaining_args == [], remaining_args\n",
        "    train(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    cli_main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MRf6HqPqy4",
        "outputId": "adf7ec5e-756b-4284-81fd-4e906e2dc750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_sz=32, data_path='/content/gdrive/My Drive/ETMC/datasets/nyud2/', LOAD_SIZE=256, FINE_SIZE=224, dropout=0.1, gradient_accumulation_steps=3, hidden=[], hidden_sz=768, img_embed_pool_type='avg', img_hidden_sz=512, include_bn=True, lr=0.0005, lr_factor=0.2, lr_patience=30, max_epochs=500, n_workers=12, name='resReleasedVersion', num_image_embeds=1, patience=30, savedir='/content/gdrive/My Drive/ETMC/results/nyud2/', seed=1, n_classes=10, annealing_epoch=10, Holder_parge=1.5, backbone='Res')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - 01/10/25 03:21:49 - 0:00:00 - FINE_SIZE: 224\n",
            "                                     Holder_parge: 1.5\n",
            "                                     LOAD_SIZE: 256\n",
            "                                     annealing_epoch: 10\n",
            "                                     backbone: Res\n",
            "                                     batch_sz: 32\n",
            "                                     data_path: /content/gdrive/My Drive/ETMC/datasets/nyud2/\n",
            "                                     dropout: 0.1\n",
            "                                     gradient_accumulation_steps: 3\n",
            "                                     hidden: []\n",
            "                                     hidden_sz: 768\n",
            "                                     img_embed_pool_type: avg\n",
            "                                     img_hidden_sz: 512\n",
            "                                     include_bn: True\n",
            "                                     lr: 0.0005\n",
            "                                     lr_factor: 0.2\n",
            "                                     lr_patience: 30\n",
            "                                     max_epochs: 500\n",
            "                                     n_classes: 10\n",
            "                                     n_workers: 12\n",
            "                                     name: resReleasedVersion\n",
            "                                     num_image_embeds: 1\n",
            "                                     patience: 30\n",
            "                                     savedir: /content/gdrive/My Drive/ETMC/results/nyud2/resReleasedVersion\n",
            "                                     seed: 1\n",
            "100%|██████████| 25/25 [00:25<00:00,  1.01s/it]\n",
            "INFO - 01/10/25 03:22:36 - 0:00:47 - Train Loss: 1.0191\n",
            "INFO - 01/10/25 03:22:36 - 0:00:47 - Test: Loss: 9.87720 | depth_acc: 0.58133, rgb_acc: 0.51205, depthrgb_acc: 0.61596\n",
            "INFO - 01/10/25 03:22:36 - 0:00:47 - Test: Loss: 9.87720 | depth_pre: 0.61621, rgb_pre: 0.54148, depthrgb_pre: 0.63503\n",
            "INFO - 01/10/25 03:22:36 - 0:00:47 - Test: Loss: 9.87720 | depth_rec: 0.58133, rgb_rec: 0.51205, depthrgb_rec: 0.61596\n",
            "INFO - 01/10/25 03:22:36 - 0:00:47 - Test: Loss: 9.87720 | depth_f1: 0.57142, rgb_f1: 0.50687, depthrgb_f1: 0.60100\n",
            "100%|██████████| 25/25 [00:28<00:00,  1.13s/it]\n",
            "INFO - 01/10/25 03:23:27 - 0:01:38 - Train Loss: 0.8840\n",
            "INFO - 01/10/25 03:23:27 - 0:01:38 - Test: Loss: 9.17287 | depth_acc: 0.59940, rgb_acc: 0.59036, depthrgb_acc: 0.66566\n",
            "INFO - 01/10/25 03:23:27 - 0:01:38 - Test: Loss: 9.17287 | depth_pre: 0.61752, rgb_pre: 0.61631, depthrgb_pre: 0.68757\n",
            "INFO - 01/10/25 03:23:27 - 0:01:38 - Test: Loss: 9.17287 | depth_rec: 0.59940, rgb_rec: 0.59036, depthrgb_rec: 0.66566\n",
            "INFO - 01/10/25 03:23:27 - 0:01:38 - Test: Loss: 9.17287 | depth_f1: 0.59577, rgb_f1: 0.58765, depthrgb_f1: 0.65816\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.05s/it]\n",
            "INFO - 01/10/25 03:24:17 - 0:02:28 - Train Loss: 0.7866\n",
            "INFO - 01/10/25 03:24:17 - 0:02:28 - Test: Loss: 9.59995 | depth_acc: 0.59789, rgb_acc: 0.54970, depthrgb_acc: 0.66867\n",
            "INFO - 01/10/25 03:24:17 - 0:02:28 - Test: Loss: 9.59995 | depth_pre: 0.62812, rgb_pre: 0.60534, depthrgb_pre: 0.69365\n",
            "INFO - 01/10/25 03:24:17 - 0:02:28 - Test: Loss: 9.59995 | depth_rec: 0.59789, rgb_rec: 0.54970, depthrgb_rec: 0.66867\n",
            "INFO - 01/10/25 03:24:17 - 0:02:28 - Test: Loss: 9.59995 | depth_f1: 0.59640, rgb_f1: 0.56726, depthrgb_f1: 0.67279\n",
            "100%|██████████| 25/25 [00:31<00:00,  1.26s/it]\n",
            "INFO - 01/10/25 03:25:10 - 0:03:20 - Train Loss: 0.9832\n",
            "INFO - 01/10/25 03:25:10 - 0:03:20 - Test: Loss: 11.70100 | depth_acc: 0.53313, rgb_acc: 0.51355, depthrgb_acc: 0.59036\n",
            "INFO - 01/10/25 03:25:10 - 0:03:20 - Test: Loss: 11.70100 | depth_pre: 0.66018, rgb_pre: 0.61315, depthrgb_pre: 0.69116\n",
            "INFO - 01/10/25 03:25:10 - 0:03:20 - Test: Loss: 11.70100 | depth_rec: 0.53313, rgb_rec: 0.51355, depthrgb_rec: 0.59036\n",
            "INFO - 01/10/25 03:25:10 - 0:03:20 - Test: Loss: 11.70100 | depth_f1: 0.55525, rgb_f1: 0.51247, depthrgb_f1: 0.59474\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.06s/it]\n",
            "INFO - 01/10/25 03:25:57 - 0:04:08 - Train Loss: 0.9818\n",
            "INFO - 01/10/25 03:25:57 - 0:04:08 - Test: Loss: 9.85595 | depth_acc: 0.53614, rgb_acc: 0.58886, depthrgb_acc: 0.65512\n",
            "INFO - 01/10/25 03:25:57 - 0:04:08 - Test: Loss: 9.85595 | depth_pre: 0.63134, rgb_pre: 0.55404, depthrgb_pre: 0.65173\n",
            "INFO - 01/10/25 03:25:57 - 0:04:08 - Test: Loss: 9.85595 | depth_rec: 0.53614, rgb_rec: 0.58886, depthrgb_rec: 0.65512\n",
            "INFO - 01/10/25 03:25:57 - 0:04:08 - Test: Loss: 9.85595 | depth_f1: 0.55315, rgb_f1: 0.55483, depthrgb_f1: 0.63922\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:26:46 - 0:04:56 - Train Loss: 1.0195\n",
            "INFO - 01/10/25 03:26:46 - 0:04:56 - Test: Loss: 10.85657 | depth_acc: 0.51355, rgb_acc: 0.45181, depthrgb_acc: 0.55271\n",
            "INFO - 01/10/25 03:26:46 - 0:04:56 - Test: Loss: 10.85657 | depth_pre: 0.55966, rgb_pre: 0.56063, depthrgb_pre: 0.61592\n",
            "INFO - 01/10/25 03:26:46 - 0:04:56 - Test: Loss: 10.85657 | depth_rec: 0.51355, rgb_rec: 0.45181, depthrgb_rec: 0.55271\n",
            "INFO - 01/10/25 03:26:46 - 0:04:56 - Test: Loss: 10.85657 | depth_f1: 0.52304, rgb_f1: 0.47617, depthrgb_f1: 0.57186\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.05s/it]\n",
            "INFO - 01/10/25 03:27:37 - 0:05:48 - Train Loss: 0.9934\n",
            "INFO - 01/10/25 03:27:37 - 0:05:48 - Test: Loss: 10.02826 | depth_acc: 0.57380, rgb_acc: 0.51958, depthrgb_acc: 0.59488\n",
            "INFO - 01/10/25 03:27:37 - 0:05:48 - Test: Loss: 10.02826 | depth_pre: 0.61227, rgb_pre: 0.55664, depthrgb_pre: 0.63226\n",
            "INFO - 01/10/25 03:27:37 - 0:05:48 - Test: Loss: 10.02826 | depth_rec: 0.57380, rgb_rec: 0.51958, depthrgb_rec: 0.59488\n",
            "INFO - 01/10/25 03:27:37 - 0:05:48 - Test: Loss: 10.02826 | depth_f1: 0.57558, rgb_f1: 0.49436, depthrgb_f1: 0.57841\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.10s/it]\n",
            "INFO - 01/10/25 03:28:27 - 0:06:37 - Train Loss: 0.9870\n",
            "INFO - 01/10/25 03:28:27 - 0:06:37 - Test: Loss: 9.71701 | depth_acc: 0.54819, rgb_acc: 0.54819, depthrgb_acc: 0.64157\n",
            "INFO - 01/10/25 03:28:27 - 0:06:37 - Test: Loss: 9.71701 | depth_pre: 0.64604, rgb_pre: 0.58194, depthrgb_pre: 0.66253\n",
            "INFO - 01/10/25 03:28:27 - 0:06:37 - Test: Loss: 9.71701 | depth_rec: 0.54819, rgb_rec: 0.54819, depthrgb_rec: 0.64157\n",
            "INFO - 01/10/25 03:28:27 - 0:06:37 - Test: Loss: 9.71701 | depth_f1: 0.56799, rgb_f1: 0.55589, depthrgb_f1: 0.64807\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.06s/it]\n",
            "INFO - 01/10/25 03:29:16 - 0:07:26 - Train Loss: 0.8988\n",
            "INFO - 01/10/25 03:29:16 - 0:07:26 - Test: Loss: 11.38221 | depth_acc: 0.45331, rgb_acc: 0.51657, depthrgb_acc: 0.54518\n",
            "INFO - 01/10/25 03:29:16 - 0:07:26 - Test: Loss: 11.38221 | depth_pre: 0.63115, rgb_pre: 0.55055, depthrgb_pre: 0.64538\n",
            "INFO - 01/10/25 03:29:16 - 0:07:26 - Test: Loss: 11.38221 | depth_rec: 0.45331, rgb_rec: 0.51657, depthrgb_rec: 0.54518\n",
            "INFO - 01/10/25 03:29:16 - 0:07:26 - Test: Loss: 11.38221 | depth_f1: 0.48972, rgb_f1: 0.52326, depthrgb_f1: 0.57360\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:30:05 - 0:08:16 - Train Loss: 0.8410\n",
            "INFO - 01/10/25 03:30:05 - 0:08:16 - Test: Loss: 9.86149 | depth_acc: 0.60542, rgb_acc: 0.51054, depthrgb_acc: 0.62651\n",
            "INFO - 01/10/25 03:30:05 - 0:08:16 - Test: Loss: 9.86149 | depth_pre: 0.64468, rgb_pre: 0.52748, depthrgb_pre: 0.64412\n",
            "INFO - 01/10/25 03:30:05 - 0:08:16 - Test: Loss: 9.86149 | depth_rec: 0.60542, rgb_rec: 0.51054, depthrgb_rec: 0.62651\n",
            "INFO - 01/10/25 03:30:05 - 0:08:16 - Test: Loss: 9.86149 | depth_f1: 0.60824, rgb_f1: 0.49311, depthrgb_f1: 0.62035\n",
            "100%|██████████| 25/25 [00:31<00:00,  1.27s/it]\n",
            "INFO - 01/10/25 03:30:59 - 0:09:10 - Train Loss: 0.8199\n",
            "INFO - 01/10/25 03:30:59 - 0:09:10 - Test: Loss: 10.01264 | depth_acc: 0.62199, rgb_acc: 0.49247, depthrgb_acc: 0.65663\n",
            "INFO - 01/10/25 03:30:59 - 0:09:10 - Test: Loss: 10.01264 | depth_pre: 0.61452, rgb_pre: 0.60662, depthrgb_pre: 0.66828\n",
            "INFO - 01/10/25 03:30:59 - 0:09:10 - Test: Loss: 10.01264 | depth_rec: 0.62199, rgb_rec: 0.49247, depthrgb_rec: 0.65663\n",
            "INFO - 01/10/25 03:30:59 - 0:09:10 - Test: Loss: 10.01264 | depth_f1: 0.60803, rgb_f1: 0.48158, depthrgb_f1: 0.64304\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:31:48 - 0:09:59 - Train Loss: 0.7703\n",
            "INFO - 01/10/25 03:31:48 - 0:09:59 - Test: Loss: 9.93834 | depth_acc: 0.53163, rgb_acc: 0.58886, depthrgb_acc: 0.61145\n",
            "INFO - 01/10/25 03:31:48 - 0:09:59 - Test: Loss: 9.93834 | depth_pre: 0.64145, rgb_pre: 0.60774, depthrgb_pre: 0.66900\n",
            "INFO - 01/10/25 03:31:48 - 0:09:59 - Test: Loss: 9.93834 | depth_rec: 0.53163, rgb_rec: 0.58886, depthrgb_rec: 0.61145\n",
            "INFO - 01/10/25 03:31:48 - 0:09:59 - Test: Loss: 9.93834 | depth_f1: 0.54241, rgb_f1: 0.57371, depthrgb_f1: 0.60750\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:32:37 - 0:10:48 - Train Loss: 0.7263\n",
            "INFO - 01/10/25 03:32:37 - 0:10:48 - Test: Loss: 10.33987 | depth_acc: 0.51355, rgb_acc: 0.56627, depthrgb_acc: 0.62199\n",
            "INFO - 01/10/25 03:32:37 - 0:10:48 - Test: Loss: 10.33987 | depth_pre: 0.62737, rgb_pre: 0.57065, depthrgb_pre: 0.63738\n",
            "INFO - 01/10/25 03:32:37 - 0:10:48 - Test: Loss: 10.33987 | depth_rec: 0.51355, rgb_rec: 0.56627, depthrgb_rec: 0.62199\n",
            "INFO - 01/10/25 03:32:37 - 0:10:48 - Test: Loss: 10.33987 | depth_f1: 0.53481, rgb_f1: 0.55927, depthrgb_f1: 0.62443\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:33:32 - 0:11:43 - Train Loss: 0.6368\n",
            "INFO - 01/10/25 03:33:32 - 0:11:43 - Test: Loss: 10.20601 | depth_acc: 0.58434, rgb_acc: 0.59036, depthrgb_acc: 0.65663\n",
            "INFO - 01/10/25 03:33:32 - 0:11:43 - Test: Loss: 10.20601 | depth_pre: 0.64016, rgb_pre: 0.62266, depthrgb_pre: 0.68932\n",
            "INFO - 01/10/25 03:33:32 - 0:11:43 - Test: Loss: 10.20601 | depth_rec: 0.58434, rgb_rec: 0.59036, depthrgb_rec: 0.65663\n",
            "INFO - 01/10/25 03:33:32 - 0:11:43 - Test: Loss: 10.20601 | depth_f1: 0.59451, rgb_f1: 0.59665, depthrgb_f1: 0.65881\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:34:22 - 0:12:33 - Train Loss: 0.5835\n",
            "INFO - 01/10/25 03:34:22 - 0:12:33 - Test: Loss: 9.44567 | depth_acc: 0.59940, rgb_acc: 0.62349, depthrgb_acc: 0.67319\n",
            "INFO - 01/10/25 03:34:22 - 0:12:33 - Test: Loss: 9.44567 | depth_pre: 0.60954, rgb_pre: 0.60177, depthrgb_pre: 0.65040\n",
            "INFO - 01/10/25 03:34:22 - 0:12:33 - Test: Loss: 9.44567 | depth_rec: 0.59940, rgb_rec: 0.62349, depthrgb_rec: 0.67319\n",
            "INFO - 01/10/25 03:34:22 - 0:12:33 - Test: Loss: 9.44567 | depth_f1: 0.58840, rgb_f1: 0.59935, depthrgb_f1: 0.65069\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:35:12 - 0:13:23 - Train Loss: 0.5579\n",
            "INFO - 01/10/25 03:35:12 - 0:13:23 - Test: Loss: 9.58110 | depth_acc: 0.62801, rgb_acc: 0.57078, depthrgb_acc: 0.68223\n",
            "INFO - 01/10/25 03:35:12 - 0:13:23 - Test: Loss: 9.58110 | depth_pre: 0.65622, rgb_pre: 0.59909, depthrgb_pre: 0.69357\n",
            "INFO - 01/10/25 03:35:12 - 0:13:23 - Test: Loss: 9.58110 | depth_rec: 0.62801, rgb_rec: 0.57078, depthrgb_rec: 0.68223\n",
            "INFO - 01/10/25 03:35:12 - 0:13:23 - Test: Loss: 9.58110 | depth_f1: 0.63074, rgb_f1: 0.57364, depthrgb_f1: 0.67988\n",
            "100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n",
            "INFO - 01/10/25 03:36:04 - 0:14:14 - Train Loss: 0.5308\n",
            "INFO - 01/10/25 03:36:04 - 0:14:14 - Test: Loss: 10.26291 | depth_acc: 0.58434, rgb_acc: 0.58434, depthrgb_acc: 0.63855\n",
            "INFO - 01/10/25 03:36:04 - 0:14:14 - Test: Loss: 10.26291 | depth_pre: 0.64277, rgb_pre: 0.59680, depthrgb_pre: 0.65917\n",
            "INFO - 01/10/25 03:36:04 - 0:14:14 - Test: Loss: 10.26291 | depth_rec: 0.58434, rgb_rec: 0.58434, depthrgb_rec: 0.63855\n",
            "INFO - 01/10/25 03:36:04 - 0:14:14 - Test: Loss: 10.26291 | depth_f1: 0.60012, rgb_f1: 0.56858, depthrgb_f1: 0.62981\n",
            "100%|██████████| 25/25 [00:33<00:00,  1.34s/it]\n",
            "INFO - 01/10/25 03:37:01 - 0:15:11 - Train Loss: 0.5416\n",
            "INFO - 01/10/25 03:37:01 - 0:15:11 - Test: Loss: 10.11829 | depth_acc: 0.60693, rgb_acc: 0.57681, depthrgb_acc: 0.63554\n",
            "INFO - 01/10/25 03:37:01 - 0:15:11 - Test: Loss: 10.11829 | depth_pre: 0.61530, rgb_pre: 0.56974, depthrgb_pre: 0.62040\n",
            "INFO - 01/10/25 03:37:01 - 0:15:11 - Test: Loss: 10.11829 | depth_rec: 0.60693, rgb_rec: 0.57681, depthrgb_rec: 0.63554\n",
            "INFO - 01/10/25 03:37:01 - 0:15:11 - Test: Loss: 10.11829 | depth_f1: 0.60355, rgb_f1: 0.56860, depthrgb_f1: 0.62269\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.10s/it]\n",
            "INFO - 01/10/25 03:37:51 - 0:16:01 - Train Loss: 0.5126\n",
            "INFO - 01/10/25 03:37:51 - 0:16:01 - Test: Loss: 10.65825 | depth_acc: 0.58735, rgb_acc: 0.57380, depthrgb_acc: 0.64910\n",
            "INFO - 01/10/25 03:37:51 - 0:16:01 - Test: Loss: 10.65825 | depth_pre: 0.64309, rgb_pre: 0.59369, depthrgb_pre: 0.66667\n",
            "INFO - 01/10/25 03:37:51 - 0:16:01 - Test: Loss: 10.65825 | depth_rec: 0.58735, rgb_rec: 0.57380, depthrgb_rec: 0.64910\n",
            "INFO - 01/10/25 03:37:51 - 0:16:01 - Test: Loss: 10.65825 | depth_f1: 0.60119, rgb_f1: 0.54919, depthrgb_f1: 0.63329\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:38:39 - 0:16:49 - Train Loss: 0.5255\n",
            "INFO - 01/10/25 03:38:39 - 0:16:49 - Test: Loss: 9.78207 | depth_acc: 0.62349, rgb_acc: 0.60392, depthrgb_acc: 0.68675\n",
            "INFO - 01/10/25 03:38:39 - 0:16:49 - Test: Loss: 9.78207 | depth_pre: 0.64693, rgb_pre: 0.58107, depthrgb_pre: 0.67105\n",
            "INFO - 01/10/25 03:38:39 - 0:16:49 - Test: Loss: 9.78207 | depth_rec: 0.62349, rgb_rec: 0.60392, depthrgb_rec: 0.68675\n",
            "INFO - 01/10/25 03:38:39 - 0:16:49 - Test: Loss: 9.78207 | depth_f1: 0.62272, rgb_f1: 0.58239, depthrgb_f1: 0.66968\n",
            "100%|██████████| 25/25 [00:33<00:00,  1.33s/it]\n",
            "INFO - 01/10/25 03:39:36 - 0:17:47 - Train Loss: 0.5418\n",
            "INFO - 01/10/25 03:39:36 - 0:17:47 - Test: Loss: 10.34967 | depth_acc: 0.58735, rgb_acc: 0.58283, depthrgb_acc: 0.67620\n",
            "INFO - 01/10/25 03:39:36 - 0:17:47 - Test: Loss: 10.34967 | depth_pre: 0.65712, rgb_pre: 0.60160, depthrgb_pre: 0.67645\n",
            "INFO - 01/10/25 03:39:36 - 0:17:47 - Test: Loss: 10.34967 | depth_rec: 0.58735, rgb_rec: 0.58283, depthrgb_rec: 0.67620\n",
            "INFO - 01/10/25 03:39:36 - 0:17:47 - Test: Loss: 10.34967 | depth_f1: 0.59669, rgb_f1: 0.57771, depthrgb_f1: 0.67078\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:40:27 - 0:18:37 - Train Loss: 0.5204\n",
            "INFO - 01/10/25 03:40:27 - 0:18:37 - Test: Loss: 10.51071 | depth_acc: 0.59187, rgb_acc: 0.56777, depthrgb_acc: 0.64458\n",
            "INFO - 01/10/25 03:40:27 - 0:18:37 - Test: Loss: 10.51071 | depth_pre: 0.66397, rgb_pre: 0.57714, depthrgb_pre: 0.67284\n",
            "INFO - 01/10/25 03:40:27 - 0:18:37 - Test: Loss: 10.51071 | depth_rec: 0.59187, rgb_rec: 0.56777, depthrgb_rec: 0.64458\n",
            "INFO - 01/10/25 03:40:27 - 0:18:37 - Test: Loss: 10.51071 | depth_f1: 0.61623, rgb_f1: 0.56561, depthrgb_f1: 0.64981\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:41:15 - 0:19:26 - Train Loss: 0.5043\n",
            "INFO - 01/10/25 03:41:15 - 0:19:26 - Test: Loss: 10.83902 | depth_acc: 0.57229, rgb_acc: 0.53163, depthrgb_acc: 0.62651\n",
            "INFO - 01/10/25 03:41:15 - 0:19:26 - Test: Loss: 10.83902 | depth_pre: 0.64009, rgb_pre: 0.51767, depthrgb_pre: 0.65185\n",
            "INFO - 01/10/25 03:41:15 - 0:19:26 - Test: Loss: 10.83902 | depth_rec: 0.57229, rgb_rec: 0.53163, depthrgb_rec: 0.62651\n",
            "INFO - 01/10/25 03:41:15 - 0:19:26 - Test: Loss: 10.83902 | depth_f1: 0.58428, rgb_f1: 0.51476, depthrgb_f1: 0.63151\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:42:07 - 0:20:17 - Train Loss: 0.5362\n",
            "INFO - 01/10/25 03:42:07 - 0:20:17 - Test: Loss: 10.68442 | depth_acc: 0.61145, rgb_acc: 0.54066, depthrgb_acc: 0.64006\n",
            "INFO - 01/10/25 03:42:07 - 0:20:17 - Test: Loss: 10.68442 | depth_pre: 0.64756, rgb_pre: 0.54443, depthrgb_pre: 0.66272\n",
            "INFO - 01/10/25 03:42:07 - 0:20:17 - Test: Loss: 10.68442 | depth_rec: 0.61145, rgb_rec: 0.54066, depthrgb_rec: 0.64006\n",
            "INFO - 01/10/25 03:42:07 - 0:20:17 - Test: Loss: 10.68442 | depth_f1: 0.61447, rgb_f1: 0.51472, depthrgb_f1: 0.62955\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:42:56 - 0:21:07 - Train Loss: 0.5340\n",
            "INFO - 01/10/25 03:42:56 - 0:21:07 - Test: Loss: 9.94453 | depth_acc: 0.61898, rgb_acc: 0.55723, depthrgb_acc: 0.66416\n",
            "INFO - 01/10/25 03:42:56 - 0:21:07 - Test: Loss: 9.94453 | depth_pre: 0.64240, rgb_pre: 0.57943, depthrgb_pre: 0.66843\n",
            "INFO - 01/10/25 03:42:56 - 0:21:07 - Test: Loss: 9.94453 | depth_rec: 0.61898, rgb_rec: 0.55723, depthrgb_rec: 0.66416\n",
            "INFO - 01/10/25 03:42:56 - 0:21:07 - Test: Loss: 9.94453 | depth_f1: 0.61812, rgb_f1: 0.55945, depthrgb_f1: 0.65563\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.05s/it]\n",
            "INFO - 01/10/25 03:43:45 - 0:21:56 - Train Loss: 0.5513\n",
            "INFO - 01/10/25 03:43:45 - 0:21:56 - Test: Loss: 10.95380 | depth_acc: 0.52711, rgb_acc: 0.55120, depthrgb_acc: 0.60392\n",
            "INFO - 01/10/25 03:43:45 - 0:21:56 - Test: Loss: 10.95380 | depth_pre: 0.65415, rgb_pre: 0.57143, depthrgb_pre: 0.66141\n",
            "INFO - 01/10/25 03:43:45 - 0:21:56 - Test: Loss: 10.95380 | depth_rec: 0.52711, rgb_rec: 0.55120, depthrgb_rec: 0.60392\n",
            "INFO - 01/10/25 03:43:45 - 0:21:56 - Test: Loss: 10.95380 | depth_f1: 0.56405, rgb_f1: 0.55547, depthrgb_f1: 0.62347\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:44:35 - 0:22:45 - Train Loss: 0.5346\n",
            "INFO - 01/10/25 03:44:35 - 0:22:45 - Test: Loss: 10.52767 | depth_acc: 0.58133, rgb_acc: 0.52108, depthrgb_acc: 0.66265\n",
            "INFO - 01/10/25 03:44:35 - 0:22:45 - Test: Loss: 10.52767 | depth_pre: 0.58993, rgb_pre: 0.56095, depthrgb_pre: 0.67114\n",
            "INFO - 01/10/25 03:44:35 - 0:22:45 - Test: Loss: 10.52767 | depth_rec: 0.58133, rgb_rec: 0.52108, depthrgb_rec: 0.66265\n",
            "INFO - 01/10/25 03:44:35 - 0:22:45 - Test: Loss: 10.52767 | depth_f1: 0.57559, rgb_f1: 0.51900, depthrgb_f1: 0.65359\n",
            "100%|██████████| 25/25 [00:32<00:00,  1.29s/it]\n",
            "INFO - 01/10/25 03:45:29 - 0:23:40 - Train Loss: 0.4933\n",
            "INFO - 01/10/25 03:45:29 - 0:23:40 - Test: Loss: 11.13227 | depth_acc: 0.59036, rgb_acc: 0.56024, depthrgb_acc: 0.64910\n",
            "INFO - 01/10/25 03:45:29 - 0:23:40 - Test: Loss: 11.13227 | depth_pre: 0.65988, rgb_pre: 0.55668, depthrgb_pre: 0.67667\n",
            "INFO - 01/10/25 03:45:29 - 0:23:40 - Test: Loss: 11.13227 | depth_rec: 0.59036, rgb_rec: 0.56024, depthrgb_rec: 0.64910\n",
            "INFO - 01/10/25 03:45:29 - 0:23:40 - Test: Loss: 11.13227 | depth_f1: 0.60130, rgb_f1: 0.54033, depthrgb_f1: 0.63797\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:46:19 - 0:24:30 - Train Loss: 0.4688\n",
            "INFO - 01/10/25 03:46:19 - 0:24:30 - Test: Loss: 11.27295 | depth_acc: 0.62048, rgb_acc: 0.54819, depthrgb_acc: 0.64458\n",
            "INFO - 01/10/25 03:46:19 - 0:24:30 - Test: Loss: 11.27295 | depth_pre: 0.62681, rgb_pre: 0.54139, depthrgb_pre: 0.62868\n",
            "INFO - 01/10/25 03:46:19 - 0:24:30 - Test: Loss: 11.27295 | depth_rec: 0.62048, rgb_rec: 0.54819, depthrgb_rec: 0.64458\n",
            "INFO - 01/10/25 03:46:19 - 0:24:30 - Test: Loss: 11.27295 | depth_f1: 0.60965, rgb_f1: 0.52205, depthrgb_f1: 0.61636\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:47:09 - 0:25:20 - Train Loss: 0.5174\n",
            "INFO - 01/10/25 03:47:09 - 0:25:20 - Test: Loss: 11.97616 | depth_acc: 0.50151, rgb_acc: 0.53163, depthrgb_acc: 0.57681\n",
            "INFO - 01/10/25 03:47:09 - 0:25:20 - Test: Loss: 11.97616 | depth_pre: 0.63314, rgb_pre: 0.56450, depthrgb_pre: 0.65348\n",
            "INFO - 01/10/25 03:47:09 - 0:25:20 - Test: Loss: 11.97616 | depth_rec: 0.50151, rgb_rec: 0.53163, depthrgb_rec: 0.57681\n",
            "INFO - 01/10/25 03:47:09 - 0:25:20 - Test: Loss: 11.97616 | depth_f1: 0.54276, rgb_f1: 0.53117, depthrgb_f1: 0.59483\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:48:03 - 0:26:14 - Train Loss: 0.4802\n",
            "INFO - 01/10/25 03:48:03 - 0:26:14 - Test: Loss: 10.50529 | depth_acc: 0.57681, rgb_acc: 0.59639, depthrgb_acc: 0.66566\n",
            "INFO - 01/10/25 03:48:03 - 0:26:14 - Test: Loss: 10.50529 | depth_pre: 0.59429, rgb_pre: 0.57552, depthrgb_pre: 0.65535\n",
            "INFO - 01/10/25 03:48:03 - 0:26:14 - Test: Loss: 10.50529 | depth_rec: 0.57681, rgb_rec: 0.59639, depthrgb_rec: 0.66566\n",
            "INFO - 01/10/25 03:48:03 - 0:26:14 - Test: Loss: 10.50529 | depth_f1: 0.57478, rgb_f1: 0.57474, depthrgb_f1: 0.64298\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.10s/it]\n",
            "INFO - 01/10/25 03:48:54 - 0:27:04 - Train Loss: 0.4956\n",
            "INFO - 01/10/25 03:48:54 - 0:27:04 - Test: Loss: 10.68355 | depth_acc: 0.60843, rgb_acc: 0.60090, depthrgb_acc: 0.67018\n",
            "INFO - 01/10/25 03:48:54 - 0:27:04 - Test: Loss: 10.68355 | depth_pre: 0.60282, rgb_pre: 0.60342, depthrgb_pre: 0.64164\n",
            "INFO - 01/10/25 03:48:54 - 0:27:04 - Test: Loss: 10.68355 | depth_rec: 0.60843, rgb_rec: 0.60090, depthrgb_rec: 0.67018\n",
            "INFO - 01/10/25 03:48:54 - 0:27:04 - Test: Loss: 10.68355 | depth_f1: 0.60186, rgb_f1: 0.58709, depthrgb_f1: 0.64810\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:49:42 - 0:27:52 - Train Loss: 0.4723\n",
            "INFO - 01/10/25 03:49:42 - 0:27:52 - Test: Loss: 11.19268 | depth_acc: 0.58133, rgb_acc: 0.53765, depthrgb_acc: 0.63554\n",
            "INFO - 01/10/25 03:49:42 - 0:27:52 - Test: Loss: 11.19268 | depth_pre: 0.61057, rgb_pre: 0.59990, depthrgb_pre: 0.66139\n",
            "INFO - 01/10/25 03:49:42 - 0:27:52 - Test: Loss: 11.19268 | depth_rec: 0.58133, rgb_rec: 0.53765, depthrgb_rec: 0.63554\n",
            "INFO - 01/10/25 03:49:42 - 0:27:52 - Test: Loss: 11.19268 | depth_f1: 0.57778, rgb_f1: 0.53700, depthrgb_f1: 0.63074\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:50:29 - 0:28:40 - Train Loss: 0.4536\n",
            "INFO - 01/10/25 03:50:29 - 0:28:40 - Test: Loss: 10.68885 | depth_acc: 0.56476, rgb_acc: 0.57078, depthrgb_acc: 0.62349\n",
            "INFO - 01/10/25 03:50:29 - 0:28:40 - Test: Loss: 10.68885 | depth_pre: 0.66993, rgb_pre: 0.56549, depthrgb_pre: 0.64360\n",
            "INFO - 01/10/25 03:50:29 - 0:28:40 - Test: Loss: 10.68885 | depth_rec: 0.56476, rgb_rec: 0.57078, depthrgb_rec: 0.62349\n",
            "INFO - 01/10/25 03:50:29 - 0:28:40 - Test: Loss: 10.68885 | depth_f1: 0.59305, rgb_f1: 0.56321, depthrgb_f1: 0.62565\n",
            "100%|██████████| 25/25 [00:31<00:00,  1.24s/it]\n",
            "INFO - 01/10/25 03:51:23 - 0:29:34 - Train Loss: 0.4500\n",
            "INFO - 01/10/25 03:51:23 - 0:29:34 - Test: Loss: 10.12871 | depth_acc: 0.60843, rgb_acc: 0.59337, depthrgb_acc: 0.67169\n",
            "INFO - 01/10/25 03:51:23 - 0:29:34 - Test: Loss: 10.12871 | depth_pre: 0.63868, rgb_pre: 0.59198, depthrgb_pre: 0.65605\n",
            "INFO - 01/10/25 03:51:23 - 0:29:34 - Test: Loss: 10.12871 | depth_rec: 0.60843, rgb_rec: 0.59337, depthrgb_rec: 0.67169\n",
            "INFO - 01/10/25 03:51:23 - 0:29:34 - Test: Loss: 10.12871 | depth_f1: 0.61190, rgb_f1: 0.59040, depthrgb_f1: 0.66024\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:52:14 - 0:30:24 - Train Loss: 0.4505\n",
            "INFO - 01/10/25 03:52:14 - 0:30:24 - Test: Loss: 10.53744 | depth_acc: 0.58886, rgb_acc: 0.60241, depthrgb_acc: 0.66114\n",
            "INFO - 01/10/25 03:52:14 - 0:30:24 - Test: Loss: 10.53744 | depth_pre: 0.64231, rgb_pre: 0.60964, depthrgb_pre: 0.68101\n",
            "INFO - 01/10/25 03:52:14 - 0:30:24 - Test: Loss: 10.53744 | depth_rec: 0.58886, rgb_rec: 0.60241, depthrgb_rec: 0.66114\n",
            "INFO - 01/10/25 03:52:14 - 0:30:24 - Test: Loss: 10.53744 | depth_f1: 0.59760, rgb_f1: 0.59218, depthrgb_f1: 0.65529\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 03:53:02 - 0:31:13 - Train Loss: 0.4635\n",
            "INFO - 01/10/25 03:53:02 - 0:31:13 - Test: Loss: 10.10208 | depth_acc: 0.61145, rgb_acc: 0.56325, depthrgb_acc: 0.67018\n",
            "INFO - 01/10/25 03:53:02 - 0:31:13 - Test: Loss: 10.10208 | depth_pre: 0.63377, rgb_pre: 0.57791, depthrgb_pre: 0.67303\n",
            "INFO - 01/10/25 03:53:02 - 0:31:13 - Test: Loss: 10.10208 | depth_rec: 0.61145, rgb_rec: 0.56325, depthrgb_rec: 0.67018\n",
            "INFO - 01/10/25 03:53:02 - 0:31:13 - Test: Loss: 10.10208 | depth_f1: 0.60756, rgb_f1: 0.56260, depthrgb_f1: 0.66314\n",
            "100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n",
            "INFO - 01/10/25 03:53:52 - 0:32:02 - Train Loss: 0.4862\n",
            "INFO - 01/10/25 03:53:52 - 0:32:02 - Test: Loss: 11.40425 | depth_acc: 0.60241, rgb_acc: 0.57078, depthrgb_acc: 0.65361\n",
            "INFO - 01/10/25 03:53:52 - 0:32:02 - Test: Loss: 11.40425 | depth_pre: 0.67790, rgb_pre: 0.60998, depthrgb_pre: 0.68580\n",
            "INFO - 01/10/25 03:53:52 - 0:32:02 - Test: Loss: 11.40425 | depth_rec: 0.60241, rgb_rec: 0.57078, depthrgb_rec: 0.65361\n",
            "INFO - 01/10/25 03:53:52 - 0:32:02 - Test: Loss: 11.40425 | depth_f1: 0.61212, rgb_f1: 0.58045, depthrgb_f1: 0.65606\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 03:54:42 - 0:32:52 - Train Loss: 0.4931\n",
            "INFO - 01/10/25 03:54:42 - 0:32:52 - Test: Loss: 11.41008 | depth_acc: 0.60693, rgb_acc: 0.50904, depthrgb_acc: 0.60542\n",
            "INFO - 01/10/25 03:54:42 - 0:32:52 - Test: Loss: 11.41008 | depth_pre: 0.64844, rgb_pre: 0.54761, depthrgb_pre: 0.63220\n",
            "INFO - 01/10/25 03:54:42 - 0:32:52 - Test: Loss: 11.41008 | depth_rec: 0.60693, rgb_rec: 0.50904, depthrgb_rec: 0.60542\n",
            "INFO - 01/10/25 03:54:42 - 0:32:52 - Test: Loss: 11.41008 | depth_f1: 0.61608, rgb_f1: 0.50311, depthrgb_f1: 0.59829\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.04s/it]\n",
            "INFO - 01/10/25 03:55:30 - 0:33:40 - Train Loss: 0.4538\n",
            "INFO - 01/10/25 03:55:30 - 0:33:40 - Test: Loss: 11.29418 | depth_acc: 0.56175, rgb_acc: 0.59036, depthrgb_acc: 0.64458\n",
            "INFO - 01/10/25 03:55:30 - 0:33:40 - Test: Loss: 11.29418 | depth_pre: 0.60653, rgb_pre: 0.60727, depthrgb_pre: 0.65794\n",
            "INFO - 01/10/25 03:55:30 - 0:33:40 - Test: Loss: 11.29418 | depth_rec: 0.56175, rgb_rec: 0.59036, depthrgb_rec: 0.64458\n",
            "INFO - 01/10/25 03:55:30 - 0:33:40 - Test: Loss: 11.29418 | depth_f1: 0.57306, rgb_f1: 0.57513, depthrgb_f1: 0.63669\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:56:20 - 0:34:30 - Train Loss: 0.4456\n",
            "INFO - 01/10/25 03:56:20 - 0:34:30 - Test: Loss: 10.78069 | depth_acc: 0.61596, rgb_acc: 0.55723, depthrgb_acc: 0.65512\n",
            "INFO - 01/10/25 03:56:20 - 0:34:30 - Test: Loss: 10.78069 | depth_pre: 0.63472, rgb_pre: 0.56936, depthrgb_pre: 0.67213\n",
            "INFO - 01/10/25 03:56:20 - 0:34:30 - Test: Loss: 10.78069 | depth_rec: 0.61596, rgb_rec: 0.55723, depthrgb_rec: 0.65512\n",
            "INFO - 01/10/25 03:56:20 - 0:34:30 - Test: Loss: 10.78069 | depth_f1: 0.61835, rgb_f1: 0.55794, depthrgb_f1: 0.65004\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:57:07 - 0:35:18 - Train Loss: 0.4537\n",
            "INFO - 01/10/25 03:57:07 - 0:35:18 - Test: Loss: 10.65697 | depth_acc: 0.62048, rgb_acc: 0.56476, depthrgb_acc: 0.65964\n",
            "INFO - 01/10/25 03:57:07 - 0:35:18 - Test: Loss: 10.65697 | depth_pre: 0.65098, rgb_pre: 0.57172, depthrgb_pre: 0.67444\n",
            "INFO - 01/10/25 03:57:07 - 0:35:18 - Test: Loss: 10.65697 | depth_rec: 0.62048, rgb_rec: 0.56476, depthrgb_rec: 0.65964\n",
            "INFO - 01/10/25 03:57:07 - 0:35:18 - Test: Loss: 10.65697 | depth_f1: 0.62920, rgb_f1: 0.55985, depthrgb_f1: 0.65564\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.05s/it]\n",
            "INFO - 01/10/25 03:57:55 - 0:36:06 - Train Loss: 0.4221\n",
            "INFO - 01/10/25 03:57:55 - 0:36:06 - Test: Loss: 11.60797 | depth_acc: 0.57530, rgb_acc: 0.58133, depthrgb_acc: 0.63253\n",
            "INFO - 01/10/25 03:57:55 - 0:36:06 - Test: Loss: 11.60797 | depth_pre: 0.65885, rgb_pre: 0.59111, depthrgb_pre: 0.65801\n",
            "INFO - 01/10/25 03:57:55 - 0:36:06 - Test: Loss: 11.60797 | depth_rec: 0.57530, rgb_rec: 0.58133, depthrgb_rec: 0.63253\n",
            "INFO - 01/10/25 03:57:55 - 0:36:06 - Test: Loss: 11.60797 | depth_f1: 0.59340, rgb_f1: 0.56027, depthrgb_f1: 0.62227\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:58:44 - 0:36:54 - Train Loss: 0.3975\n",
            "INFO - 01/10/25 03:58:44 - 0:36:54 - Test: Loss: 11.12148 | depth_acc: 0.57831, rgb_acc: 0.54367, depthrgb_acc: 0.63855\n",
            "INFO - 01/10/25 03:58:44 - 0:36:54 - Test: Loss: 11.12148 | depth_pre: 0.62725, rgb_pre: 0.55062, depthrgb_pre: 0.65475\n",
            "INFO - 01/10/25 03:58:44 - 0:36:54 - Test: Loss: 11.12148 | depth_rec: 0.57831, rgb_rec: 0.54367, depthrgb_rec: 0.63855\n",
            "INFO - 01/10/25 03:58:44 - 0:36:54 - Test: Loss: 11.12148 | depth_f1: 0.58978, rgb_f1: 0.54183, depthrgb_f1: 0.64086\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 03:59:38 - 0:37:49 - Train Loss: 0.4219\n",
            "INFO - 01/10/25 03:59:38 - 0:37:49 - Test: Loss: 11.39255 | depth_acc: 0.56777, rgb_acc: 0.51054, depthrgb_acc: 0.62651\n",
            "INFO - 01/10/25 03:59:38 - 0:37:49 - Test: Loss: 11.39255 | depth_pre: 0.59233, rgb_pre: 0.56417, depthrgb_pre: 0.63845\n",
            "INFO - 01/10/25 03:59:38 - 0:37:49 - Test: Loss: 11.39255 | depth_rec: 0.56777, rgb_rec: 0.51054, depthrgb_rec: 0.62651\n",
            "INFO - 01/10/25 03:59:38 - 0:37:49 - Test: Loss: 11.39255 | depth_f1: 0.56520, rgb_f1: 0.52511, depthrgb_f1: 0.62655\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n",
            "INFO - 01/10/25 04:00:28 - 0:38:38 - Train Loss: 0.4016\n",
            "INFO - 01/10/25 04:00:28 - 0:38:38 - Test: Loss: 11.54289 | depth_acc: 0.56024, rgb_acc: 0.58133, depthrgb_acc: 0.63102\n",
            "INFO - 01/10/25 04:00:28 - 0:38:38 - Test: Loss: 11.54289 | depth_pre: 0.61062, rgb_pre: 0.58862, depthrgb_pre: 0.63745\n",
            "INFO - 01/10/25 04:00:28 - 0:38:38 - Test: Loss: 11.54289 | depth_rec: 0.56024, rgb_rec: 0.58133, depthrgb_rec: 0.63102\n",
            "INFO - 01/10/25 04:00:28 - 0:38:38 - Test: Loss: 11.54289 | depth_f1: 0.55915, rgb_f1: 0.56955, depthrgb_f1: 0.61488\n",
            "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
            "INFO - 01/10/25 04:01:17 - 0:39:27 - Train Loss: 0.4064\n",
            "INFO - 01/10/25 04:01:17 - 0:39:27 - Test: Loss: 11.47703 | depth_acc: 0.57229, rgb_acc: 0.55873, depthrgb_acc: 0.62500\n",
            "INFO - 01/10/25 04:01:17 - 0:39:27 - Test: Loss: 11.47703 | depth_pre: 0.63339, rgb_pre: 0.60209, depthrgb_pre: 0.65803\n",
            "INFO - 01/10/25 04:01:17 - 0:39:27 - Test: Loss: 11.47703 | depth_rec: 0.57229, rgb_rec: 0.55873, depthrgb_rec: 0.62500\n",
            "INFO - 01/10/25 04:01:17 - 0:39:27 - Test: Loss: 11.47703 | depth_f1: 0.58527, rgb_f1: 0.55596, depthrgb_f1: 0.62291\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "INFO - 01/10/25 04:02:07 - 0:40:17 - Train Loss: 0.4040\n",
            "INFO - 01/10/25 04:02:07 - 0:40:17 - Test: Loss: 11.37989 | depth_acc: 0.57982, rgb_acc: 0.57380, depthrgb_acc: 0.66265\n",
            "INFO - 01/10/25 04:02:07 - 0:40:17 - Test: Loss: 11.37989 | depth_pre: 0.62930, rgb_pre: 0.60500, depthrgb_pre: 0.68616\n",
            "INFO - 01/10/25 04:02:07 - 0:40:17 - Test: Loss: 11.37989 | depth_rec: 0.57982, rgb_rec: 0.57380, depthrgb_rec: 0.66265\n",
            "INFO - 01/10/25 04:02:07 - 0:40:17 - Test: Loss: 11.37989 | depth_f1: 0.58440, rgb_f1: 0.56842, depthrgb_f1: 0.65938\n",
            "100%|██████████| 25/25 [00:31<00:00,  1.27s/it]\n",
            "INFO - 01/10/25 04:03:02 - 0:41:13 - Train Loss: 0.4045\n",
            "INFO - 01/10/25 04:03:02 - 0:41:13 - Test: Loss: 11.60175 | depth_acc: 0.57229, rgb_acc: 0.58735, depthrgb_acc: 0.63554\n",
            "INFO - 01/10/25 04:03:02 - 0:41:13 - Test: Loss: 11.60175 | depth_pre: 0.56638, rgb_pre: 0.57491, depthrgb_pre: 0.63154\n",
            "INFO - 01/10/25 04:03:02 - 0:41:13 - Test: Loss: 11.60175 | depth_rec: 0.57229, rgb_rec: 0.58735, depthrgb_rec: 0.63554\n",
            "INFO - 01/10/25 04:03:02 - 0:41:13 - Test: Loss: 11.60175 | depth_f1: 0.54630, rgb_f1: 0.56299, depthrgb_f1: 0.60473\n",
            "100%|██████████| 25/25 [00:27<00:00,  1.10s/it]\n",
            "INFO - 01/10/25 04:03:51 - 0:42:02 - Train Loss: 0.4280\n",
            "INFO - 01/10/25 04:03:51 - 0:42:02 - Test: Loss: 11.64773 | depth_acc: 0.57681, rgb_acc: 0.58735, depthrgb_acc: 0.64307\n",
            "INFO - 01/10/25 04:03:51 - 0:42:02 - Test: Loss: 11.64773 | depth_pre: 0.61238, rgb_pre: 0.57549, depthrgb_pre: 0.64586\n",
            "INFO - 01/10/25 04:03:51 - 0:42:02 - Test: Loss: 11.64773 | depth_rec: 0.57681, rgb_rec: 0.58735, depthrgb_rec: 0.64307\n",
            "INFO - 01/10/25 04:03:51 - 0:42:02 - Test: Loss: 11.64773 | depth_f1: 0.57866, rgb_f1: 0.55499, depthrgb_f1: 0.61779\n",
            "INFO - 01/10/25 04:03:52 - 0:42:03 - No improvement. Breaking out of loop.\n",
            "INFO - 01/10/25 04:04:14 - 0:42:25 - Test: Loss: 9.78207 | depth_acc: 0.62349, rgb_acc: 0.60392, depth rgb acc: 0.68675\n",
            "INFO - 01/10/25 04:04:14 - 0:42:25 - Test: Loss: 9.78207 | depth_pre: 0.64693, rgb_pre: 0.58107, depth rgb pre: 0.67105\n",
            "INFO - 01/10/25 04:04:14 - 0:42:25 - Test: Loss: 9.78207 | depth_rec: 0.62349, rgb_rec: 0.60392, depth rgb rec: 0.68675\n",
            "INFO - 01/10/25 04:04:14 - 0:42:25 - Test: Loss: 9.78207 | depth_f1: 0.62272, rgb_f1: 0.58239, depth rgb f1: 0.66968\n",
            "INFO - 01/10/25 04:04:14 - 0:42:25 - Test: Loss: 9.78207 | depth_acc: 0.62349, rgb_acc: 0.60392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNzCqp3DPo0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}